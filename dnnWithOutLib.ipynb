{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "R_Jn2vLMdnLJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.special"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8I57O3aheC6r"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "feature = pd.DataFrame(iris['data'],columns=iris['feature_names'])\n",
        "target = pd.DataFrame(iris['target'],columns=['class'])\n",
        "\n",
        "df = pd.concat([feature,target],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "qfy1nJEghvL6"
      },
      "outputs": [],
      "source": [
        "x = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,4].values\n",
        "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1WpQ8pzHkcW6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# use fit_transform to standardization\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "i-3ktykWb41j"
      },
      "outputs": [],
      "source": [
        "class MultiLayerNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate):\n",
        "        self.activation = lambda x: scipy.special.expit(x)\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights = [np.random.normal(0.0, pow(layer_sizes[i], -0.5), (layer_sizes[i], layer_sizes[i - 1]))\n",
        "                        for i in range(1, self.num_layers)]\n",
        "\n",
        "    def train(self, inputs, targets):\n",
        "        inputs = np.array(inputs, ndmin=2).T\n",
        "        targets = np.array(targets, ndmin=2).T\n",
        "\n",
        "        # Forward pass\n",
        "        layer_outputs = [inputs]\n",
        "        for i in range(self.num_layers - 2):\n",
        "            layer_input = np.dot(self.weights[i], layer_outputs[-1])\n",
        "            layer_output = self.activation(layer_input)\n",
        "            layer_outputs.append(layer_output)\n",
        "\n",
        "        final_input = np.dot(self.weights[-1], layer_outputs[-1])\n",
        "        final_output = self.activation(final_input)\n",
        "        layer_outputs.append(final_output)\n",
        "\n",
        "        # Backward pass\n",
        "        errors = [targets - final_output]\n",
        "        for i in range(self.num_layers - 2, 0, -1):\n",
        "            hidden_errors = np.dot(self.weights[i].T, errors[-1])\n",
        "            errors.append(hidden_errors)\n",
        "\n",
        "        # Update weights\n",
        "        for i in range(self.num_layers - 2, -1, -1):\n",
        "            self.weights[i] += self.lr * np.dot((errors[-(i + 1)] * layer_outputs[i + 1] * (1 - layer_outputs[i + 1])),\n",
        "                                                np.transpose(layer_outputs[i]))\n",
        "\n",
        "    def query(self, inputs):\n",
        "        inputs = np.array(inputs, ndmin=2).T\n",
        "\n",
        "        # Forward pass\n",
        "        for i in range(self.num_layers - 2):\n",
        "            layer_input = np.dot(self.weights[i], inputs)\n",
        "            inputs = self.activation(layer_input)\n",
        "\n",
        "        final_input = np.dot(self.weights[-1], inputs)\n",
        "        final_output = self.activation(final_input)\n",
        "        return final_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "BOvzh8C3z1ou"
      },
      "outputs": [],
      "source": [
        "nn2 = MultiLayerNeuralNetwork([4,100,10,3],0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcNqGJc10Fud"
      },
      "outputs": [],
      "source": [
        "epoche = 1000\n",
        "performance = []\n",
        "for e in range(epoche):\n",
        "  scorecard = []\n",
        "  for i, data in enumerate(X_train):\n",
        "    input = data\n",
        "    target = np.zeros(3)+0.01\n",
        "    target[int(y_train[i])]  = 0.99\n",
        "    nn2.train(input,target)\n",
        "  for i, data in enumerate(X_test):\n",
        "    Ans = int(y_test[i])\n",
        "    input = data\n",
        "    output = nn2.query(input)\n",
        "    predict = np.argmax(output)\n",
        "    if predict == Ans:\n",
        "      scorecard.append(1)\n",
        "    else:\n",
        "      scorecard.append(0)\n",
        "  scorecard_array = np.asarray(scorecard)\n",
        "  performance.append(scorecard_array.sum()/scorecard_array.size)\n",
        "plt.plot(performance)\n",
        "plt.title('Neural Network Performance over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
